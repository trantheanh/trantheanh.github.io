---
title: "[Blueprint – 3] Enriching Word Vectors with Subword Information"
date: 2020-07-18
layout: post
comments: true
mathjax: true
---

# Enriching Word Vectors with Subword Information

## 1. Morphological word representation: (Biểu diễn hình thái học của từ)

English: love, lover, loving

Vietnamese:  mảnh_mai, mảnh_dẻ, mảnh_khảnh

=> Nếu không có biểu diễn hình thái học của từ vựng, những từ dù có chung hình thái cũng sẽ không liên quan gì tới nhau

=> Giải pháp: 

- Character Level features
- Subword level features

## 2. Model:

- ### General model:

  - **Skip-gram**: (Given word, predict context)

    - Corpus = $[w_1, \ldots, w_T] $

    - context = $C_t$ (set of indices of words surrounding word $w_t$)

    - objective: 

      - maximum  
        $$
        \sum^T_{t=1}\sum_{c\in C_t}log \space p(w_c \mid w_t)
        $$

    - trong đó: 
      $$
      p(w_c \mid w_t) = \frac{e^{s(w_t, w_c)}}{\sum^W_{j=1}e^{s(w_t, j)}}
      $$
      (s : **score function** maps **pair** (word, context))

      

    Tuy nhiên model của tác giả chỉ dự đoán 1 context word $w_c$ .

    

  - **Negative Sampling**: 

  Vì bài toán dự đoán context word (tất cả context word) có thể được thay thế bằng một tập các bài toán phân loại nhị phân (dự đoán 1 context word). Khi đó mục tiêu của bài toán trở thành dự đoán sự có măt hay không của context word. 

  - các cặp ($w_t, w_c$)  trong đó $w_c \in C_t$ , $w_t$ là word thứ $t$ trong corpus
    - các cặp ($w_t, n$) trong đó $n \in N_{t,c}$, với $N_{t,c}$  là tập những word không thuộc context của $w_t$ được sample từ từ điển (vocab)

  

  Cross-entropy giữa phân phối $p$ và phân phối $q$: $plog(q)$

  => Binary logistic loss (negative cross-entropy): $-y\space log\space p(x) - (1-y)\space log \space (1-p(x))$ 

  => Tương ứng với dữ liệu: 

  - Positive pair $(y=1)$ : 
    $$
    -1 \space log \frac{1}{1 + e^{-s(w_t, w_c)}} = -(log(1) - log(1 + e^{-s(w_t, w_c)})) = log(1 + e^{-s(w_t, w_c)})
    $$

  - Negative pairs $(y = 0)$ : 
    $$
      -1 \space log(1 - \frac{1}{1 + e^{-s(w_t, n)}}) = log(1 + e^{s(w_t, n)})
    $$


    Nhưng vì 1 cặp positive có tới $N_{t, c}$ cặp negative nên loss trên negative pair trở thành: 


    $$
      \sum_{n \in N_{t,c}} log(1 + e^{s(w_t, n)})
    $$

  => **negative log-likelihood**:    
  $$
    log(1 + e^{-s(w_t, w_c)}) + \sum_{n \in N_{t,c}} log(1 + e^{s(w_t, n)})
  $$
    Ký hiệu: $ℓ: x\to log(1+e^{-x})$ 

  => **Objective function**: 
  $$
    \sum^{T}_{t=1} \left[ \sum_{c \in C_t}ℓ(s(w_t, w_c)) + \sum_{n \in N_{t,c}} ℓ(-s(w_t, n))   \right]
  $$

  - Hàm tính score $s$ ở đây là tích vô hướng. Trong đó word tại vị trí $t$ trong corpus $w_t$ được biểu diễn bằng vector input $u_w$ (embedding của original word) còn $w_c$ (context của $w_t$) được biểu diễn bởi vector output $v_c$ (embedding của context word). 

  $$
    s(w_t, w_c) = u_{w_t}^T v_{w_c}
  $$


  - Để hiểu hơn về embedding của original word (input embedding) và embedding của context word (output embedding) ta nên xem paper của 

    [word2vec]: https://arxiv.org/pdf/1301.3781.pdf

- ### Subword model:

  - Những mô hình trên sử dụng embedding ở mức word, do đó chúng bỏ qua những thông tin nằm chính trong cấu trúc của từ đó. Chẳng hạn trong từ l-o-v-e-r nội dung của phần l-o-v-e có ý nghĩa giống như trong những từ l-o-v-e-l-y, hay phần l-o-v có ý nghĩa giống như trong từ l-o-v-i-n-g. Vì vậy trong bài báo, có đề cập đến một score function $s$ khác, trong đó hàm này sẽ quan tâm tới những thông tin trên (subword information).

  - "where" với 3-gram:

    - character 3-gram: "<wh", "whe", "her", "ere", "re>"
    - word: "<where\>"

  - Thực tế tác giả sử dụng tất cả n-gram từ 3->6 cùng lúc

  - tập n-gram của word $w$ : $G_w$ 

  - score function: 


    $$
    s(w, c) = \sum_{g \in G_w} z_g^Tv_c
    $$
    trong đó $z_g$ là biểu diễn n-gram của $w$ 

## 3. Training:

- Optimization:

  - **Optimizer**: Stochastic Gradient Descent

  - **loss**: negative log likelihood

  - **learning rate**: linear decay of step side

    - n_examples = T

    - n_epoch = P

    - step = $t$

    - $lr = \gamma_0 (1 - \frac{t}{TP}) $

      step $t$ tính theo $batch\_size = 1$ nên với $batch\_size > 1$ có thể coi $step = t * batch\_size$ (số data example đã được duyệt qua)

      Skip-gram: $\gamma_0 = 0.025$

      Cbow: $\gamma_0 = 0.05$

      Subword model: $\gamma_0 = 0.05$

- Implementation details:

  - word vector dimension size = 300
  - 1 positive example => sample 5 negative example

- Datasets:

  - using context window $c$ => sampling from $c \in [1,5]$ 

  - Loại bỏ các từ với xác suất như sau:

    $$P(w_i) = 1 - \sqrt{\frac{t}{f(w_i)}} $$  

    Trong đó:

    - $f(w_i)$ là tần suất xuất hiện của word $w_i$ 
    - $t=10^{-4}$ là ngưỡng ta chọn trước 

## 4. Word similarity for OOV words:

- Vì các từ được cấu thành từ subword nên ngay cả khi từ mới không nằm trong từ điển, ta vẫn có thể biểu diễn những từ này một cách đơn giản với trung bình tất cả các n-gram vector của chúng.