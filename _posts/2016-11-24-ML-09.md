---
title: "[ML ‚Äì 09] Understanding Neural Network 3/3"
date: 2016-11-24
layout: post
comments: true
mathjax: true
---

Xin ch√†o c√°c b·∫°n, h√¥m nay ch√∫ng ta ti·∫øp t·ª•c lo·∫°t b√†i v·ªÅ ANN. Trong b√†i vi·∫øt tr∆∞·ªõc ch√∫ng ta ƒë√£ ƒëi v√†o v√≠ d·ª• c·ª• th·ªÉ xem ANN v·∫≠n h√†nh nh∆∞ th·∫ø n√†o, v√† n√≥ c√≥ l·ª£i h∆°n so v·ªõi Logistic Regression ra sao ƒë·ªìng th·ªùi c≈©ng ƒë√£ b∆∞·ªõc ƒë·∫ßu t∆∞·ªüng t∆∞·ª£ng ƒë∆∞·ª£c ANN h·ªçc nh∆∞ th·∫ø n√†o. Khi √°p d·ª•ng gi·∫£i thu·∫≠t Gradient Descent v·ªõi y√™u c·∫ßu c·∫ßn t√≠nh ƒë·∫°o h√†m t·∫°i t·∫•t c·∫£ c√°c tham s·ªë \\(\theta\\) hay n√≥i c√°ch kh√°c l√† x√°c ƒë·ªãnh xem m·ªói tham s·ªë \\(\theta\\) ·∫£nh h∆∞·ªüng t·ªõi cost function nh∆∞ th·∫ø n√†o. ·ªû b√†i tr∆∞·ªõc ta ƒë√£ bi·∫øt 2 c√°ch t√≠nh ƒë·∫°o h√†m v√† th·∫•y ƒë∆∞·ª£c r·∫±ng c√°ch t√≠nh ng∆∞·ª£c ƒë·∫°o h√†m c·ªßa cost function \\(J(\theta)\\) t·∫°i m·ªói node hay m·ªói neuron s·∫Ω gi·∫£m thi·ªÉu chi ph√≠ t√≠nh to√°n ƒëi r·∫•t nhi·ªÅu (khi ANN c√†ng ph·ª©c t·∫°p). B√¢y gi·ªù ch√∫ng ta s·∫Ω xem, n·∫øu √°p d·ª•ng c·ª• th·ªÉ, gi·∫£i thu·∫≠t s·∫Ω nh∆∞ th·∫ø n√†o ?

## 1. Cost function:

ƒê·ªëi v·ªõi ANN th√¨ cost function ph·ª• thu·ªôc v√†o h√†m activation c·ªßa layer cu·ªëi c√πng (Output Layer), m√† trong tr∆∞·ªùng h·ª£p c·ª• th·ªÉ xuy√™n su·ªët 3 b√†i vi·∫øt v·ªÅ ANN n√†y th√¨ m·ªçi h√†m activation ƒë·ªÅu l√† **Sigmoid Function.** V√¨ v·∫≠y n√™n cost function s·∫Ω c√≥ d·∫°ng t∆∞∆°ng t·ª± **Logistic Regression**: 

\\[
J(\theta) = -\frac{1}{m} (\sum_{i=1}^{m}y^{(i)}log(h_{\theta}(x^{(i)})) + (1-y^{(i)})log(1 - h_{\theta}(x^{(i)}))) 
\\]

ƒê·ªëi v·ªõi ANN c√≥ k output hay d√πng ƒë·ªÉ ph√¢n lo·∫°i \\(\mathbf{k}\\) label kh√°c nhau th√¨ cost function c√≥ d·∫°ng: 

\\[h_{\theta}(x) \in R^{k}\\
(h_{\theta}(x))_{k} = k^{th} output\\]

\\[ J(\theta) = -\frac{1}{m}[\sum_{i=1}^{m}\sum_{k=1}^{k} y_{k}^{(i)} log(h_{\theta}(x^{(i)}))_k + (1 - y^{(i)}) log(1 - (h _{\theta} (x^{(i)})) _{k})] \\]

Trong ƒë√≥ ·ªü ƒë√¢y \\(\mathbf{m}\\) l√† s·ªë l∆∞·ª£ng training example trong training set, c√≤n \\(\mathbf{k}\\) l√† s·ªë l∆∞·ª£ng output hay s·ªë l∆∞·ª£ng label. ƒê·ªÉ ƒë∆°n gi·∫£n, tr∆∞·ªõc ti√™n ta s·∫Ω l√†m vi·ªác v·ªõi training set ch·ªâ c√≥ 1 training example duy nh·∫•t v√† m·ªói layer ch·ªâ c√≥ 1 neuron. Cost function v√† m·∫°ng neuron trong v√≠ d·ª• h√¥m nay s·∫Ω nh∆∞ sau: 

\\[J(\theta) = [ ylog(h_{\theta}(x)) + (1-y)log(1-h_{\theta}(x)) ]\\]

<img src="/assets/content_images/ml09-01.png" alt = "" width = "100%">

M·ª•c ti√™u c·ªßa ta l√† t√≠nh ƒë·∫°o h√†m t·∫°i m·ªói tham s·ªë \\(\mathbf{\theta}\\), ch·∫≥ng h·∫°n v·ªõi \\(\theta_{11}^{(1)}\\): 

<img src="/assets/content_images/ml09-02.png" alt = "" width = "100%">

## 2. Back Propagation Algorithm:

Gi·ªù ch√∫ng ta xem \\(\theta_{11}^{(1)}\\) ·∫£nh h∆∞·ªüng t·ªõi c√°c neuron nh∆∞ th·∫ø n√†o ?
ƒê·∫ßu ti√™n l√† \\(a_{1}^{(2)}\\)

<img src="/assets/content_images/ml09-03.png" alt = "" width = "100%">

Ti·∫øp theo l√† \\(a_{1}^{(3)}\\)

<img src="/assets/content_images/ml09-04.png" alt = "" width = "100%">

Ti·∫øp t·ª•c v·ªõi \\(a_{1}^{(4)}\\)

<img src="/assets/content_images/ml09-05.png" alt = "" width = "100%">

·ªû ƒë√¢y \\(a_{1}^{(4)}\\) ch√≠nh l√† \\(h_{\theta}(x)\\). M√† trong b√†i v·ªÅ Logistic Regression ch√∫ng ta ƒë√£ t√≠nh ra ƒë∆∞·ª£c ƒë·∫°o h√†m ri√™ng t·∫°i m·ªói \\(\theta\\) c√≥ d·∫°ng: 
$$
\begin{align*}
&\space \space J(\theta)
 = \Bigg[ y \space log \space h_\theta(x) + (1-y) \space log(1 - h_\theta(x)) \Bigg]
\\
\Rightarrow & \space J'(\theta) = (h(x) - y) * \frac{\triangle z}{\triangle \theta} \space \space \space \space \space \space (z = \theta^Tx)
\\
\\
\Rightarrow & \space J'(\theta) = (h(x) - y) * x_j
\end{align*}
$$





<img src="/assets/content_images/ml09-06.png" alt = "" width = "100%">

N√™n ƒë·∫°o h√†m t·∫°i \\(z_{1}^{(4)}\\), hay \\(z_{1}^{(4)}\\) ·∫£nh h∆∞·ªüng t·ªõi cost function l√† \\(a_{1}^{(4)} ‚Äì y\\) (ch√≠nh l√† \\(h_{\theta}(x) ‚Äì y)\\). V√¨ khi ƒë√≥ ƒë·∫°o h√†m ri√™ng c·ªßa \\(J\\) t·∫°i \\(z_{1}^{(4)}\\) s·∫Ω c√≥ ph·∫ßn ƒë·∫°o h√†m c·ªßa \\(z\\) tr√™n \\(\theta\\) b·∫±ng \\(1\\). Ch√≠nh v√¨ v·∫≠y ta kh√¥ng c·∫ßn t√≠nh to√°n \\(\theta\\) ·∫£nh h∆∞·ªüng t·ªõi \\(a_{1}^{(4)}\\) m√† ch·ªâ c·∫ßn t√≠nh \\(\theta\\) ·∫£nh h∆∞·ªüng t·ªõi \\(z_{1}^{(4)}\\). Ta b·ªè ƒëi ph·∫ßn ƒë·∫°o h√†m tr√™n \\(a_{1}^{(4)}\\) s·∫Ω c√≥: 

$$
\begin{align*}

\frac{\Delta z_{1}^{4}}{\Delta \theta_{11}^{1}}
& = \frac{\Delta z_1^{(4)}}{\Delta a_1^{(3)}} 
* \frac{\Delta a_1^{(3)}}{\Delta z_1^{(3)}} 
* \frac{\Delta z_1^{(3)}}{\Delta a_1^{(2)}} 
* \frac{\Delta a_1^{(2)}}{\Delta z_1^{(2)}} 
* \frac{\Delta z_1^{(2)}}{\Delta \theta _{11}^{(1)}}\\
& = \theta _{11}^{(3)} * g'(z_1^{(3)}) * \theta _{11}^{2} * g'(z_1^{(2)}) * a_1^{(1)}

\end{align*}
$$

N·∫øu nh∆∞ ta th√™m 1 feature x2 ta s·∫Ω c√≥ k·∫øt qu·∫£ t∆∞∆°ng t·ª± khi t√≠nh ƒë·∫°o h√†m ri√™ng t·∫°i \\(\theta_{12}^{(1)}\\): 

$$
\begin{align*}

\frac{\Delta z_{1}^{4}}{\Delta \theta_{12}^{1}}
& = \frac{\Delta z_1^{(4)}}{\Delta a_1^{(3)}} 
* \frac{\Delta a_1^{(3)}}{\Delta z_1^{(3)}} 
* \frac{\Delta z_1^{(3)}}{\Delta a_1^{(2)}} 
* \frac{\Delta a_1^{(2)}}{\Delta z_1^{(2)}} 
* \frac{\Delta z_1^{(2)}}{\Delta \theta _{11}^{(1)}}\\
& = \theta _{11}^{(3)} * g'(z_1^{(3)}) * \theta _{12}^{2} * g'(z_1^{(2)}) * a_2^{(1)}

\end{align*}
$$

Nh∆∞ v·∫≠y n·∫øu nh∆∞ ta mu·ªën t√≠nh ƒë·∫°o h√†m ri√™ng t·∫°i m·ªôt \\(\mathbf{\theta}\\) b·∫•t k√¨, ta coi nh∆∞ ANN b·∫Øt ƒë·∫ßu t·ª´ neuron ƒë√≥ v√† t√≠nh t∆∞∆°ng t·ª± nh∆∞ tr√™n, th√¨ m·ªçi \\(\mathbf{\theta}\\) c·ªßa c√πng m·ªôt layer s·∫Ω c√≥ ƒë·∫°o h√†m ri√™ng g·∫ßn nh∆∞ gi·ªëng nhau tr·ª´ th√†nh ph·∫ßn \\(a_j^{(1)}\\) l√† neuron n√≥ li√™n k·∫øt (Neuron b·∫Øt ƒë·∫ßu, \\(\theta_{ij}^{‚Ñì}\\) li√™n k·∫øt t·ª´ neuron \\(j\\) ·ªü layer \\(‚Ñì\\) t·ªõi neuron \\(i\\) ·ªü layer \\(‚Ñì+1\\) ). Quan s√°t v√≠ d·ª• tr√™n s·∫Ω th·∫•y r·∫•t r√µ v·ªõi 2 feature \\(x_1\\) v√† \\(x_2\\). B√™n c·∫°nh ƒë√≥ n·∫øu ƒë·ªÉ √Ω m·ªôt ch√∫t l√† c√°c ph·∫ßn \\(Œ∏_{11}^{(i)} * g'(z_1^{(i)})\\) l·∫∑p l·∫°i khi ta t√≠nh qua m·ªói layer.

<img src="/assets/content_images/ml09-07.png" alt = "" width = "100%">

V·∫≠y n·∫øu ta t√≠nh ng∆∞·ª£c l·∫°i k·∫øt qu·∫£ s·∫Ω nh∆∞ sau: 

$$
\begin{align*}

\frac{\Delta z_1^{(4)}}{\Delta \theta _{11}^{(1)}}
& = \theta _{11}^{3} * g'(z_1^{(3)}) * \theta _{11}^{2} * g'(z_1^{(2)}) * a_1^{(1)}\\
\frac{\Delta z_1^{(3)}}{\Delta \theta _{11}^{(1)}}
& = * \theta _{11}^{2} * g'(z_1^{(2)}) * a_1^{(1)}\\
\frac{\Delta z_1^{(2)}}{\Delta \theta _{11}^{(1)}}
& = a_1^{(1)}\\

\end{align*}
$$

Hay:

$$
\begin{align*}

\frac{\Delta z_1^{(4)}}{\Delta \theta_{11}^{(1)}}
& = \frac{\Delta z_1^{(4)}}{\Delta z_{1}^{(3)}} * 
\frac{\Delta z_1^{(3)}}{\Delta \theta_{11}^{(1)}}\\
& = \frac{\Delta z_1^{(4)}}{\Delta a_{1}^{(3)}} *
\frac{\Delta a_1^{(3)}}{\Delta z_{1}^{(3)}} * 
\frac{\Delta z_1^{(3)}}{\Delta \theta_{11}^{(1)}}\\
& = \theta _{11}^{(2)} * g'(z_1^{(2)}) * \frac{\Delta z_1^{(3)}}{\Delta \theta _{11}^{(1)}}

\end{align*}
$$

Gi·ªù ch√∫ng ta s·∫Ω t·ªïng qu√°t ho√° qu√° tr√¨nh n√†y, v√† t·∫°o ra m·ªôt ƒë·∫°i l∆∞·ª£ng m·ªõi \\(\mathbf{\delta} \\) ƒë·ªÉ ti·ªán cho vi·ªác t√≠nh to√°n. \\(\mathbf{\delta} \\) ƒë∆∞·ª£c g·ªçi l√† sai s·ªë (error) t·∫°i m·ªói neuron c√≥ gi√° tr·ªã ƒë∆∞·ª£c t√≠nh nh∆∞ sau:

\\[ \delta _j^{(‚Ñì)} = \frac{\Delta J}{\Delta z_j^{(‚Ñì)}}\\]

C·ª• th·ªÉ l√† error c·ªßa neuron th·ª© j tr√™n layer \\(‚Ñì\\). V·ªõi v√≠ d·ª• tr√™n, error c·ªßa neuron th·ª© 1 c·ªßa layer 4 l√† 

\\[ \delta _j^{(4)} = \frac{\Delta J}{\Delta z_j^{(4)}} = a_1^{(4)} - y\\]

t∆∞∆°ng t·ª± v·ªõi c√°c neurons kh√°c: 

\\[ \delta_1^{(3)} = \frac{\Delta J}{\Delta z_1^{(3)}} = \frac{\Delta J}{\Delta z_1^{4}} * \frac{\Delta z_1^{(4)}}{\Delta z_1^{(3)}} = \delta_1^{(4)} * \theta _{11}^{(3)} * g'(z_1^{(3)}) \\]

\\[ \delta_1^{(2)} = \frac{\Delta J}{\Delta z_1^{(2)}} = \frac{\Delta J}{\Delta z_1^{3}} * \frac{\Delta z_1^{(3)}}{\Delta z_1^{(2)}} = \delta_1^{(3)} * \theta _{11}^{(2)} * g'(z_1^{(2)}) \\]

\\(\delta _1^{(1)} = !!!\\) kh√¥ng c√≥ sai s·ªë do ƒë√¢y l√† input

V·ªõi m·ªói layer c√≥ s·ªë l∆∞·ª£ng neuron l·ªõn h∆°n 1 th√¨ sai s·ªë c·ªßa neuron c·ªßa m·ªôt layer s·∫Ω ƒë∆∞·ª£c t√≠nh th√¥ng qua t·∫•t c·∫£ c√°c neuron c·ªßa layer li·ªÅn k·∫ø ti·∫øp n√≥! (Kh√¥ng t√≠nh bias) 

N·∫øu vi·∫øt d∆∞·ªõi d·∫°ng vector, layer \\(‚Ñì\\) s·∫Ω c√≥ error l√† vector \\(\delta(‚Ñì)\\), c√≤n weight hay tham s·ªë t·ª´ neuron \\(j\\) layer \\(‚Ñì\\) t·ªõi layer \\(‚Ñì + 1\\) s·∫Ω l√† vector \\(\theta _j^‚Ñì\\), c·ª• th·ªÉ: 

\\[ \delta _j^{(‚Ñì)} = (\theta _j^{(‚Ñì)})^{T}\delta ^{(‚Ñì + 1)} * g'(z_j^{(‚Ñì)}) \\]

c√≤n ƒë·ªÉ t√≠nh error cho c·∫£ layer th√¨ k·∫øt qu·∫£ l√†: 
\\[ \delta ^{(‚Ñì)} = (\theta ^{(‚Ñì)})^{T}\delta ^{(‚Ñì + 1)}\space .* \space g'(z^{(‚Ñì)}) \\]

Trong ƒë√≥ tham s·ªë c·ªßa t·∫•t c·∫£ neuron trong m·ªôt layer kh√¥ng c√≤n l√† m·ªôt vector nh∆∞ tr√™n n·ªØa m√† l√† m·ªôt matrix nh∆∞ ta ƒë√£ n√≥i trong ph·∫ßn 1, matrix \\(Œ∏\\) c√≥ k√≠ch th∆∞·ªõc \\((s_{‚Ñì+1})\\)x\\((s_‚Ñì + 1)\\) v√† \\(\delta^{‚Ñì+1}\\) l√† vector \\(s_‚Ñì+1\\) chi·ªÅu. B√™n c·∫°nh ƒë√≥ ph√©p to√°n ‚Äù \\(. *\\) ‚Äù l√† l·∫•y t·ª´ng ph·∫ßn t·ª≠ c·ªßa vector thu ƒë∆∞·ª£c sau khi nh√¢n matrix \\(Œ∏\\) v√† vector \\(\delta\\) v·ªõi c√°c ph·∫ßn t·ª≠ t∆∞∆°ng ·ª©ng c·ªßa vector \\(g'(z^{(‚Ñì)})\\) (ƒê√¢y l√† ph√©p to√°n c√≥ trong MathLab, n·∫øu c√°c b·∫°n c√≥ kinh nghi·ªám v·ªÅ n√≥ th√¨ ch·∫Øc h·∫≥n s·∫Ω r·∫•t quen thu·ªôc).
V√† gi·ªù ƒë·ªÉ t√≠nh ƒë·∫°o h√†m cho c√°c \\(Œ∏\\), ta ch·ªâ c·∫ßn: 

\\[ \frac{\Delta J}{\Delta \theta_{ij}^{(‚Ñì)}} = \frac{\Delta J}{\Delta z_i^{(‚Ñì + 1)}} * \frac{\Delta z_i^{(‚Ñì + 1)}}{\Delta \theta_{ij}^{(‚Ñì)}} = \delta^{(‚Ñì + 1)} * a_j^{(‚Ñì)}\\]

ƒê·∫øn ƒë√¢y th√¨ ch√∫ng ta ƒë√£ hi·ªÉu ƒë∆∞·ª£c c√°ch t√≠nh ƒë·∫°o h√†m ri√™ng t·∫°i c√°c tham s·ªë \\(\theta\\). V·ªõi vi·ªác t√≠nh to√°n \\(\delta\\) ng∆∞·ª£c t·ª´ output layer gi√∫p gi·∫£m thi·ªÉu chi ph√≠ t√≠nh to√°n v√† thu·∫≠n ti·ªán trong vi·ªác chuy·ªÉn ƒë·ªïi t∆∞∆°ng ·ª©ng sang ƒë·∫°o h√†m ri√™ng c·ªßa \\(\theta\\). V·∫≠y l√† c√°c b·∫°n ƒë√£ n·∫Øm ƒë∆∞·ª£c gi·∫£i thu·∫≠t Back Propagation, v·ªõi ƒë·∫°o h√†m ri√™ng thu ƒë∆∞·ª£c, √°p d·ª•ng v√†o gi·∫£i thu·∫≠t Gradient Descent ta s·∫Ω t·ªëi ∆∞u ho√° ƒë∆∞·ª£c cost function v√† ANN ƒë√£ h·ªçc th√†nh c√¥ng üôÇ